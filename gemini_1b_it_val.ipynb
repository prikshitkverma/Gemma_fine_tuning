{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prikshitkverma/Gemma_fine_tuning/blob/main/gemini_1b_it_val.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiclC14XxcyG"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 1. INSTALL LIBRARIES\n",
        "# ============================================\n",
        "#!pip uninstall -y torch torchvision torchaudio transformers trl accelerate datasets huggingface_hub\n",
        "#!pip install -q torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "#!pip install -q transformers==4.45.2 trl==0.11.6 accelerate==1.1.1 datasets==3.1.0 huggingface_hub==0.28.1 sentencepiece pyarrow==18.0.0 evaluate tensorboard\n",
        "!pip uninstall -y torch torchvision torchaudio transformers trl accelerate datasets huggingface_hub\n",
        "!pip install -q torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q transformers==4.45.2 trl==0.11.6 accelerate==1.1.1 datasets==3.1.0 huggingface_hub==0.28.1 sentencepiece pyarrow==18.0.0\n",
        "!pip install -q datasets trl sentencepiece huggingface_hub\n",
        "!pip install evaluate\n",
        "# ============================================\n",
        "# 2. SETUP AND AUTHENTICATION\n",
        "# ============================================\n",
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from huggingface_hub import login\n",
        "import evaluate\n",
        "\n",
        "# Login to Hugging Face\n",
        "HF_TOKEN = \"hf_token_here\"\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "# ============================================\n",
        "# 3. CONFIGURE MODEL AND DIRECTORIES\n",
        "# ============================================\n",
        "base_model = \"google/gemma-3-1b-it\"\n",
        "output_dir = \"./gemma-natural-farming-qa\"\n",
        "\n",
        "# ============================================\n",
        "# 4. LOAD AND PREPARE THE DATASET\n",
        "# ============================================\n",
        "data_file = \"/content/natural_farming_dataset_perplexity.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files=data_file, split=\"train\")\n",
        "\n",
        "def format_dataset(sample):\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
        "            {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "formatted_dataset = dataset.map(format_dataset, remove_columns=dataset.features)\n",
        "\n",
        "# Split into 80% train, 10% validation, 10% test\n",
        "split_dataset = formatted_dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
        "val_test_split = split_dataset[\"test\"].train_test_split(test_size=0.5, shuffle=True, seed=42)\n",
        "\n",
        "dataset_dict = {\n",
        "    \"train\": split_dataset[\"train\"],\n",
        "    \"validation\": val_test_split[\"train\"],\n",
        "    \"test\": val_test_split[\"test\"]\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Dataset Split Summary:\")\n",
        "print(f\"Train: {len(dataset_dict['train'])} | Validation: {len(dataset_dict['validation'])} | Test: {len(dataset_dict['test'])}\")\n",
        "print(\"\\nExample data sample:\")\n",
        "print(dataset_dict[\"train\"][0][\"messages\"])\n",
        "\n",
        "# ============================================\n",
        "# 5. LOAD MODEL AND TOKENIZER\n",
        "# ============================================\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "print(f\"‚úÖ Model loaded on {model.device} | dtype: {model.dtype}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SsnZCoBvb11A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF6Y9KRCxd1r"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 6. CONFIGURE THE TRAINING PROCESS\n",
        "# ============================================\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,\n",
        "    # max_seq_length=256, # Removed max_seq_length\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    fp16=False,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# 7. TRAIN AND VALIDATE MODEL\n",
        "# ============================================\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=dataset_dict[\"train\"],\n",
        "    eval_dataset=dataset_dict[\"validation\"],\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"üöÄ Starting fine-tuning...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"üíæ Saving final model...\")\n",
        "trainer.save_model(output_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cChwtXcxxf_5"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_score\n",
        "# ============================================\n",
        "# 8. VALIDATE MODEL PERFORMANCE\n",
        "# ============================================\n",
        "print(\"\\nüîç Validating model on validation set...\")\n",
        "model.eval()\n",
        "\n",
        "# Use a simple text-generation pipeline\n",
        "val_pipe = pipeline(\"text-generation\", model=output_dir, tokenizer=tokenizer)\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "generated_texts = []\n",
        "reference_texts = []\n",
        "\n",
        "for i, sample in enumerate(dataset_dict[\"validation\"]):\n",
        "    user_msg = [{\"role\": \"user\", \"content\": sample[\"messages\"][0][\"content\"]}]\n",
        "    prompt = tokenizer.apply_chat_template(user_msg, tokenize=False, add_generation_prompt=True)\n",
        "    output = val_pipe(prompt, max_new_tokens=128, num_return_sequences=1)[0][\"generated_text\"][len(prompt):].strip()\n",
        "    generated_texts.append(output)\n",
        "    reference_texts.append(sample[\"messages\"][1][\"content\"])\n",
        "    if i < 3:\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Q: {sample['messages'][0]['content']}\")\n",
        "        print(f\"Model: {output}\")\n",
        "        print(f\"Ref: {sample['messages'][1]['content']}\")\n",
        "\n",
        "# Compute metrics\n",
        "bleu_score = bleu.compute(predictions=generated_texts, references=reference_texts)\n",
        "rouge_score = rouge.compute(predictions=generated_texts, references=reference_texts)\n",
        "\n",
        "print(\"\\nüìä Validation Metrics:\")\n",
        "print(f\"BLEU Score: {bleu_score['bleu']:.4f}\")\n",
        "print(f\"ROUGE-L: {rouge_score['rougeL']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "results = bertscore.compute(predictions=generated_texts, references=reference_texts, lang=\"en\")\n",
        "print(sum(results[\"f1\"]) / len(results[\"f1\"]))"
      ],
      "metadata": {
        "id": "CUaPJ06zZuzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# 9. TEST INTERACTIVELY\n",
        "# ============================================\n",
        "print(\"\\n--- Interactive Testing ---\")\n",
        "test_pipe = pipeline(\"text-generation\", model=output_dir, tokenizer=tokenizer)\n",
        "\n",
        "while True:\n",
        "    question = input(\"\\nEnter your question (or type 'exit' to quit): \").strip()\n",
        "    if question.lower() == \"exit\":\n",
        "        print(\"üëã Exiting...\")\n",
        "        break\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    outputs = test_pipe(prompt, max_new_tokens=256)\n",
        "    print(f\"\\nüß† Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
      ],
      "metadata": {
        "id": "swYbM1_NZyZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import shutil, os\n",
        "\n",
        "# Path to your best checkpoint\n",
        "best_ckpt = \"/content/gemma-natural-farming-qa/checkpoint-1200\"\n",
        "\n",
        "# Where to save clean offline model\n",
        "save_dir = \"./best_model\"\n",
        "\n",
        "# Load model and tokenizer from the checkpoint\n",
        "model = AutoModelForCausalLM.from_pretrained(best_ckpt)\n",
        "tokenizer = AutoTokenizer.from_pretrained(best_ckpt)\n",
        "\n",
        "# Save only what's needed for inference\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "print(f\"‚úÖ Saved minimal offline model to {save_dir}\")\n"
      ],
      "metadata": {
        "id": "Zw8YvPiVxrHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "import shutil\n",
        "\n",
        "# 1Ô∏è‚É£ Authenticate\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# 2Ô∏è‚É£ Zip your model folder\n",
        "local_folder = \"/content/best_model\"\n",
        "zip_path = \"/content/best_model.zip\"\n",
        "shutil.make_archive(zip_path.replace('.zip',''), 'zip', local_folder)\n",
        "print(f\"‚úÖ Zipped folder to {zip_path}\")\n",
        "\n",
        "# 3Ô∏è‚É£ Upload zip to specific Drive folder by ID\n",
        "folder_id = \"1v7wyPcLmawtlKgFsOqoMdcB8qMZ7fqPj\"\n",
        "file_metadata = {\n",
        "    'name': 'best_model.zip',\n",
        "    'parents': [folder_id]\n",
        "}\n",
        "media = MediaFileUpload(zip_path, mimetype='application/zip')\n",
        "file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
        "\n",
        "print(f\"‚úÖ Uploaded zip to Drive folder! File ID: {file.get('id')}\")\n"
      ],
      "metadata": {
        "id": "1tuzQ8974zqu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMKxiCaSocAenJWQcPeEwDQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}